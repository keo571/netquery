# Netquery Query Evaluation Framework

This document describes the comprehensive evaluation system for testing and validating the Netquery text-to-SQL pipeline using `testing/evaluate_queries.py`.

## Purpose

The Query Evaluation framework tests the complete text-to-SQL pipeline end-to-end, measuring technical success across different query types and complexity levels.

### What It Tests

#### Query Categories (Aligned with SAMPLE_QUERIES.md)

1. **Basic Queries** - Simple table queries and filtering
   - "Show me all load balancers"
   - "List servers in maintenance"
   - "What SSL certificates do we have?"

2. **Analytics & Aggregations** - Counting, statistics, performance metrics
   - "How many load balancers do we have?"
   - "What's the average CPU utilization by datacenter?"
   - "Count servers by status"

3. **Multi-Table Joins** - Complex relationships and filtering
   - "Show load balancers with their backend servers"
   - "List unhealthy load balancers with high CPU servers"

4. **Time-Series & Visualization** - Charts and trend analysis
   - "Show network traffic trends over time"
   - "Display load balancer health scores over time"

5. **Troubleshooting** - Current status and problem identification
   - "Show certificates expiring in 30 days"
   - "Which servers have connection issues?"

6. **Edge Cases & Error Handling** - Invalid queries and safety validation
   - "Delete all servers" (should be blocked)
   - "Show me nonexistent table data"

### Metrics

#### Technical Success Rate
Percentage of queries that execute successfully through the entire pipeline:
```
Technical Success Rate = (Successful Queries / Total Queries) Ã— 100%
```


### Pipeline Stage Tracking

Each query is evaluated through all pipeline stages:

| Stage | Success Indicator | What It Measures |
|-------|------------------|------------------|
| **Schema** | âœ…/âŒ | Schema analysis and table selection |
| **SQL** | âœ…/âŒ | SQL query generation |
| **Execution** | âœ…/âŒ | Query execution and result retrieval |
| **Charts** | Type/None | Automatic chart generation |

### Output

- **Console Report** - Real-time progress and summary statistics
- **HTML Report** - Detailed results table saved to `testing/evaluations/query_evaluation_report.html`

### Usage

```bash
# Run complete batch evaluation (all 49 queries with HTML report)
python testing/evaluate_queries.py

# Test a single query (pass/fail only, console output)
python testing/evaluate_queries.py --single "Show me all load balancers"

# Check GEMINI_API_KEY is set
export GEMINI_API_KEY=your_key_here
python testing/evaluate_queries.py
```

**Example Batch Output:**
```
ðŸš€ Starting Netquery Evaluation...
ðŸ“Š Testing 49 queries across 14 categories

ðŸ“‚ Basic Queries (5 queries)
   1. Testing: Show me all load balancers
      âœ… SUCCESS (1.2s, 50 rows) [bar chart]

ðŸ“ˆ EVALUATION SUMMARY
Technical Success Rate: 42/49 (85.7%)
Charts Generated: 18
HTML report saved: testing/evaluations/query_evaluation_report.html
```

**Example Single Query Output:**
```
ðŸ§ª Single Query Test Mode
Testing: "Show me all load balancers"
âœ… PASS (1.4s, 50 rows) [bar chart generated]
```

## Integration with Development

### Pre-commit Hooks
```bash
# Add to .git/hooks/pre-commit
python testing/evaluate_queries.py
if [ $? -ne 0 ]; then
    echo "Query evaluation failed"
    exit 1
fi
```

### CI/CD Pipeline
```yaml
# Add to GitHub Actions
- name: Run Query Evaluation
  run: python testing/evaluate_queries.py
  env:
    GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
```

### Performance Monitoring
- Track success rates over time
- Monitor query execution times
- Identify performance regressions
- Benchmark new features

---

## Files and Locations

- **Query Evaluation Script**: `testing/evaluate_queries.py`
- **Test Queries**: Defined in `docs/SAMPLE_QUERIES.md`
- **Reports**: Saved to `testing/evaluations/`
- **Configuration**: Uses `.env` for API keys

## Future Improvements

### Technical Success Rate Improvements

**Difficulty-Based LLM Selection**
- Use advanced LLMs (gemini-2.5-pro) for high-complexity queries identified by query planner
- Keep standard LLM (gemini-2.0-flash) for simple queries to optimize costs

**Schema Analysis Enhancement** 
- Upgrade embedding model from `all-MiniLM-L6-v2` to `text-embedding-3-large`
- Add hybrid search combining semantic similarity with BM25 scoring (BM25 provides lexical matching for exact technical terms while embeddings handle conceptual understanding)
- Enhance table descriptions with infrastructure-specific keywords

**SQL Generation Resilience**
- Implement SQL validation with retry using increased reasoning levels
- Add few-shot learning with curated network infrastructure examples
- Automatic query optimization (LIMIT clauses, execution time controls)

### Error Recovery and Reliability

**Smart Error Recovery**
- Query decomposition for overly complex requests
- Intelligent fallback to simpler query variants when complex joins fail
- Better error messages with suggested alternatives

**Learning System**
- Track query success patterns to improve complexity assessment
- Log execution failures to identify common problem areas
- Adaptive difficulty scoring based on historical performance

### Measurement Enhancements

**Additional Metrics**
- Complex query success rate (difficulty > 0.8)
- Average query execution time by complexity level
- Schema analysis accuracy by table type
- Failure mode categorization (syntax, logic, timeout, safety)

The query evaluation framework ensures Netquery maintains high quality and reliability across all text-to-SQL use cases and deployment scenarios.
